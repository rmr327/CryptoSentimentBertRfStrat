{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading training data from object store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "qb = QuantBook()\n",
    "path = qb.ObjectStore.GetFilePath(\"group_4_crypto_trading_with_sentiment_sprin_2024/final_indicators_df.csv\")\n",
    "df = pd.read_csv(path)\n",
    "df = df.set_index('Unnamed: 0')\n",
    "\n",
    "# Filter the data to only include the dates in training set\n",
    "df = df[df.index >= '2021-04-15']\n",
    "df = df[df.index < '2024-04-12']\n",
    "df = df.ffill()\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting returns to 15 day moving average\n",
    "df['15_day_moving_avg_return'] = df['return'].rolling(window=15).mean()\n",
    "df = df.dropna(subset=['15_day_moving_avg_return'])\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "def train_evaluate_and_plot(X_train, y_train, X_test, y_test, plot_title):\n",
    "    \"\"\"Train, evaluate and plot a random forest model.\"\"\"\n",
    "    model = RandomForestRegressor()\n",
    "    # fit model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # evaluate model\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "    r_squared = model.score(X_test, y_test)\n",
    "    print(f\"R Squared: {r_squared}\")\n",
    "\n",
    "    # make a dataframe with y_test and y_pred\n",
    "    results = pd.DataFrame({\"y_test\": y_test, \"y_pred\": y_pred})\n",
    "\n",
    "    # compute residuals\n",
    "    results[\"residuals\"] = results[\"y_test\"] - results[\"y_pred\"]\n",
    "\n",
    "    # Set the style of seaborn\n",
    "    sns.set_style(\"darkgrid\")\n",
    "\n",
    "    # plot results\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.plot(results[\"y_test\"], label=\"Actual_15DAR\", linewidth=2)\n",
    "    plt.plot(results[\"y_pred\"], label=\"Predicted_15DAR\", linewidth=2)\n",
    "    plt.plot(results[\"y_test\"].rolling(15).mean(), label=\"15_SMA\", linewidth=2)\n",
    "    plt.legend(loc=\"upper center\")\n",
    "    plt.title(plot_title, fontsize=16)\n",
    "    plt.xticks(rotation=90)\n",
    "    days_fmt = mdates.DateFormatter(\"%Y-%m-%d\")\n",
    "    plt.gca().xaxis.set_major_formatter(days_fmt)\n",
    "    # Set x-axis major ticks to occur every 15 days.\n",
    "    plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=15))\n",
    "    plt.ylabel(\"BTC Returns (percentage change)\", fontsize=14)\n",
    "\n",
    "\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "    # feature importance\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "    for f in range(X_train.shape[1]):\n",
    "        print(f\"{f + 1}. {X_train.columns[indices[f]]} ({importances[indices[f]]})\")\n",
    "\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    plt.title(\"Feature importances\", fontsize=16)\n",
    "    plt.bar(\n",
    "        range(X_train.shape[1]), importances[indices], align=\"center\", color=\"skyblue\"\n",
    "    )\n",
    "    plt.xticks(range(X_train.shape[1]), np.array(X_train.columns)[indices], rotation=90)\n",
    "    plt.xlim([-1, X_train.shape[1]])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# implementing the random forest model\n",
    "X = df.drop(\n",
    "    columns=[\n",
    "        \"15_day_moving_avg_return\",\n",
    "        \"return\",\n",
    "    ]\n",
    ")\n",
    "y = df[\"15_day_moving_avg_return\"]\n",
    "\n",
    "\n",
    "train_start = \"2022-04-10\"\n",
    "train_end = \"2024-03-12\"\n",
    "test_start = \"2021-04-10\"\n",
    "test_end = \"2021-12-10\"\n",
    "\n",
    "X_train = X[(X.index >= train_start) & (X.index <= train_end)]\n",
    "y_train = y[(y.index >= train_start) & (y.index <= train_end)]\n",
    "X_train.index = pd.to_datetime(X_train.index)\n",
    "y_train.index = pd.to_datetime(y_train.index)\n",
    "\n",
    "X_test = X[(X.index >= test_start) & (X.index <= test_end)]\n",
    "y_test = y[(y.index >= test_start) & (y.index <= test_end)]\n",
    "X_test.index = pd.to_datetime(X_test.index)\n",
    "y_test.index = pd.to_datetime(y_test.index)\n",
    "\n",
    "model, results = train_evaluate_and_plot(\n",
    "    X_train, y_train, X_test, y_test, \"Basline Random Forest Model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate metrics\n",
    "results[\"actual_direction\"] = results[\"y_test\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "results[\"Pred_direction\"] = results[\"y_pred\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "tp = results[\n",
    "    (results[\"actual_direction\"] == 1) & (results[\"Pred_direction\"] == 1)\n",
    "].shape[0]\n",
    "fp = results[\n",
    "    (results[\"actual_direction\"] == 0) & (results[\"Pred_direction\"] == 1)\n",
    "].shape[0]\n",
    "fn = results[\n",
    "    (results[\"actual_direction\"] == 1) & (results[\"Pred_direction\"] == 0)\n",
    "].shape[0]\n",
    "tn = results[\n",
    "    (results[\"actual_direction\"] == 0) & (results[\"Pred_direction\"] == 0)\n",
    "].shape[0]\n",
    "\n",
    "print(f\"True Positives: {tp}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"True Negatives: {tn}\")\n",
    "\n",
    "# calculate precision\n",
    "precision = tp / (tp + fp)\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "# calculate recall\n",
    "recall = tp / (tp + fn)\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "# calculate f1 score\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Feature Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of lags\n",
    "n_lags = 3\n",
    "\n",
    "# Create a new DataFrame for lagged features\n",
    "lagged_features = []\n",
    "lagged_features_cols = []\n",
    "\n",
    "X_now = df.drop(\n",
    "    columns=[\n",
    "        \"15_day_moving_avg_return\",\n",
    "        \"return\",\n",
    "    ]\n",
    ")\n",
    "y_now = df[\"15_day_moving_avg_return\"]\n",
    "\n",
    "# Create lagged versions of features\n",
    "for column in X_now.columns:\n",
    "    for lag in range(1, n_lags + 1):\n",
    "        lagged_features.append(X_now[column].rolling(lag * 10).sum())\n",
    "        lagged_features_cols.append(f\"{column}_rolling_{lag * 10}\")\n",
    "\n",
    "lagged_features = pd.concat(lagged_features, axis=1)\n",
    "lagged_features.columns = lagged_features_cols\n",
    "\n",
    "# Concatenate the original DataFrame with the DataFrame of lagged features\n",
    "X_now = pd.concat([X_now, lagged_features], axis=1)\n",
    "\n",
    "# Drop rows with NaN values caused by lagging in both X and y\n",
    "X_now = X_now.dropna()\n",
    "y_now = y_now[X_now.index]\n",
    "\n",
    "X_train = X_now[(X_now.index >= train_start) & (X_now.index <= train_end)]\n",
    "y_train = y_now[(y_now.index >= train_start) & (y_now.index <= train_end)]\n",
    "X_train.index = pd.to_datetime(X_train.index)\n",
    "y_train.index = pd.to_datetime(y_train.index)\n",
    "\n",
    "X_test = X_now[(X_now.index >= test_start) & (X_now.index <= test_end)]\n",
    "y_test = y_now[(y_now.index >= test_start) & (y_now.index <= test_end)]\n",
    "X_test.index = pd.to_datetime(X_test.index)\n",
    "y_test.index = pd.to_datetime(y_test.index)\n",
    "\n",
    "# Train, evaluate, and plot the model\n",
    "model, results = train_evaluate_and_plot(\n",
    "    X_train, y_train, X_test, y_test, \"Fine Tuned Random Forest Model\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make another model with best features from the above random forest model\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "cols = np.array(X_train.columns)[indices][:30]\n",
    "X = X_now[cols]\n",
    "y = y_now\n",
    "\n",
    "X_train = X[(X.index >= train_start) & (X.index <= train_end)]\n",
    "y_train = y[(y.index >= train_start) & (y.index <= train_end)]\n",
    "X_train.index = pd.to_datetime(X_train.index)\n",
    "y_train.index = pd.to_datetime(y_train.index)\n",
    "\n",
    "X_test = X[(X.index >= test_start) & (X.index <= test_end)]\n",
    "y_test = y[(y.index >= test_start) & (y.index <= test_end)]\n",
    "X_test.index = pd.to_datetime(X_test.index)\n",
    "y_test.index = pd.to_datetime(y_test.index)\n",
    "\n",
    "model, results = train_evaluate_and_plot(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    \"Fine Tuned Random Forest Model with Best Features\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate metrics\n",
    "results[\"actual_direction\"] = results[\"y_test\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "results[\"Pred_direction\"] = results[\"y_pred\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "tp = results[\n",
    "    (results[\"actual_direction\"] == 1) & (results[\"Pred_direction\"] == 1)\n",
    "].shape[0]\n",
    "fp = results[\n",
    "    (results[\"actual_direction\"] == 0) & (results[\"Pred_direction\"] == 1)\n",
    "].shape[0]\n",
    "fn = results[\n",
    "    (results[\"actual_direction\"] == 1) & (results[\"Pred_direction\"] == 0)\n",
    "].shape[0]\n",
    "tn = results[\n",
    "    (results[\"actual_direction\"] == 0) & (results[\"Pred_direction\"] == 0)\n",
    "].shape[0]\n",
    "\n",
    "print(f\"True Positives: {tp}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print(f\"True Negatives: {tn}\")\n",
    "\n",
    "# calculate precision\n",
    "precision = tp / (tp + fp)\n",
    "print(f\"Precision: {precision}\")\n",
    "\n",
    "# calculate recall\n",
    "recall = tp / (tp + fn)\n",
    "print(f\"Recall: {recall}\")\n",
    "\n",
    "# calculate f1 score\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Visualize a sample tree (until depth 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Plot the first tree from the forest\n",
    "plt.figure(figsize=(15, 10))\n",
    "plot_tree(model.estimators_[0], filled=True, max_depth=2, feature_names=cols)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
